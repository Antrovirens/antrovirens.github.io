
<!DOCTYPE html>
<html lang="简体中文" class="loading">
<head><meta name="generator" content="Hexo 3.9.0">
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta name="viewport" content="width=device-width, minimum-scale=1.0, maximum-scale=1.0, user-scalable=no">
    <title>神经网络从经典到卷积 - Atrovirens&#39; Café</title>
    <meta name="apple-mobile-web-app-capable" content="yes">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <meta name="google" content="notranslate">
    <meta name="keywords" content="Antrovirens,"> 
    <meta name="description" content="


神经网络是一种模拟人脑神经结构的计算机程序结构，以期能够实现人工智能的机器学习技术。本文将介绍神经网络背后的概念，并介绍如何通过编程实现神经网络。
 神经网络基础
机器可以迅速做出大量的算术运,"> 
    <meta name="author" content="antrovirens"> 
    <link rel="alternative" href="atom.xml" title="Atrovirens&#39; Café" type="application/atom+xml"> 
    <link rel="icon" href="/img/favicon.png"> 
    <link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.css">
    <link rel="stylesheet" href="/css/diaspora.css">
    <script async src="//pagead2.googlesyndication.com/pagead/js/adsbygoogle.js"></script>
    <script>
         (adsbygoogle = window.adsbygoogle || []).push({
              google_ad_client: "ca-pub-8691406134231910",
              enable_page_level_ads: true
         });
    </script>
    <script async custom-element="amp-auto-ads" src="https://cdn.ampproject.org/v0/amp-auto-ads-0.1.js">
    </script>

    <link href="https://cdn.bootcss.com/KaTeX/0.7.1/katex.min.css" rel="stylesheet">
</head>
</html>
<body class="loading">
    <span id="config-title" style="display:none">Atrovirens&#39; Café</span>
    <div id="loader"></div>
    <div id="single">
    <div id="top" style="display: block;">
    <div class="bar" style="width: 0;"></div>
    <a class="icon-home image-icon" href="javascript:;" data-url="http://yoursite.com"></a>
    <div title="播放/暂停" class="icon-play"></div>
    <h3 class="subtitle">神经网络从经典到卷积</h3>
    <div class="social">
        <!--<div class="like-icon">-->
            <!--<a href="javascript:;" class="likeThis active"><span class="icon-like"></span><span class="count">76</span></a>-->
        <!--</div>-->
        <div>
            <div class="share">
                <a title="获取二维码" class="icon-scan" href="javascript:;"></a>
            </div>
            <div id="qr"></div>
        </div>
    </div>
    <div class="scrollbar"></div>
</div>

    <div class="section">
        <div class="article">
    <div class='main'>
        <h1 class="title">神经网络从经典到卷积</h1>
        <div class="stuff">
            <span>七月 21, 2019</span>
            
  <ul class="post-tags-list"><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/卷积神经网络/">卷积神经网络</a></li><li class="post-tags-list-item"><a class="post-tags-list-link" href="/tags/神经网络/">神经网络</a></li></ul>


        </div>
        <div class="content markdown">
            <div align="center">
<img src="https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/post/190721/cover.jpg" alt="卷积神经网络">
</div>
<p>神经网络是一种模拟人脑神经结构的计算机程序结构，以期能够实现人工智能的机器学习技术。本文将介绍神经网络背后的概念，并介绍如何通过编程实现神经网络。</p>
<h2 id="神经网络基础"><a class="markdownIt-Anchor" href="#神经网络基础"></a> 神经网络基础</h2>
<p>机器可以迅速做出大量的算术运算，但是无法处理图像中包含的大量信息。我们怀疑图像识别需要特别的人类智能， 而这是机器所缺乏的，人工智能所讨论的问题正是如此。</p>
<h3 id="简单的学习器"><a class="markdownIt-Anchor" href="#简单的学习器"></a> 简单的学习器</h3>
<p>一台基本的机器的工作流程，如下图所示，接受了一个问题并作出相应的思考，得到一个输出结果。</p>
<div align="center">
<img src="https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/post/190721/simple%20learning%20mechane.jpg" alt="简单的学习器" width="300" height="100">
</div>
<p>所有有用的计算机系统都应该有一个输入和输出，并在输入和输出之间进行某种类型的计算。我们不能精确知道一些事务如何运行，但我们可以通过模型来估计其运作方式。改进这些模型的方法是通过输出值和真实值之间的比较得到偏移值，进一步调整参数。</p>
<p>以一个简单的分类器为例子，有二维坐标内有两个点系，聚集在两片区域，先要用一条直线将这两类特征分开，即为通过判断在直线的那一端来对这两类进行分类。</p>
<p>我们设计一条经过原点的直线 y = A * x 作为我们分类的依据。这条直线应该基于具体的坐标数值特征将两组点分割开来。</p>
<p>为简化工作，将实例简化为以下的表格。</p>
<table>
<thead>
<tr>
<th style="text-align:center">实例</th>
<th style="text-align:center">x坐标</th>
<th style="text-align:center">y坐标</th>
<th style="text-align:center">类别</th>
</tr>
</thead>
<tbody>
<tr>
<td style="text-align:center">1</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">A</td>
</tr>
<tr>
<td style="text-align:center">2</td>
<td style="text-align:center">1.0</td>
<td style="text-align:center">3.0</td>
<td style="text-align:center">B</td>
</tr>
</tbody>
</table>
<div align="center">
<img src="https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/post/190721/Figure_1.png" alt="线性分类器">
</div>
<p>参数A控制着直线的斜率，我们给A赋予一个初值0.25，取x = 3.0, y = 0.75，我们所期望的直线应该经过A类别的上方，比如是1.1的目标值，我们可以利用这个误差来调整A值：</p>
<blockquote>
<p>误差值 = 期望目标值 - 实际输出值<br>
E = 1.1 - 0.75 = 0.35</p>
</blockquote>
<p>分类器的线性函数应该修改为 t = (A + dA) * x</p>
<blockquote>
<p>E = t - y = (A + dA) * x - A * x = dA * x<br>
dA = E / x = 0.35 / 3 = 0.1167</p>
</blockquote>
<p>这样A的新值化为A + dA =0.3667。 接下来可以接着输入B类的样本，使得A值进一步趋向于使直线略微低于B类点群的下方。但是这样的分类方法最终改进的直线不会顾及开始的训练值，而是仅仅考虑了最近的一组数据，这不是我们想要的结果。</p>
<p>机器学习中，我们应该进行适度改进（moderate），不应该改进过于激烈，当训练数据本身不能确定为完全正确，包含现实误差和噪声的时候，可以抑制这些影响因素。<br>
在改进的公式中引入一个调节系数L，也是所谓的学习率（learning rate）</p>
<blockquote>
<p>dA = L * (E / x)</p>
</blockquote>
<p>选取L = 0.5 作为一个合理的系数，意味着我们只会更新值的一半。</p>
<p>一个简单的py实现样例代码如下。需要手工用文本写一个输入数据的文件classification cal sample.txt</p>
<figure class="highlight py"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">datas = []  <span class="comment">#初始化</span></span><br><span class="line">file_object = open(<span class="string">'classification cal sample.txt'</span>,<span class="string">'r'</span>)</span><br><span class="line"><span class="keyword">try</span>:</span><br><span class="line">    <span class="keyword">for</span> line <span class="keyword">in</span> file_object:</span><br><span class="line">        <span class="comment"># do_somthing_with(line) #line带"\n"</span></span><br><span class="line">        line = line.replace(<span class="string">','</span>,<span class="string">' '</span>).replace(<span class="string">'\n'</span>,<span class="string">''</span>)  <span class="comment">#去除无用字符</span></span><br><span class="line">        num = list(map(float,line.split()))  <span class="comment"># 把空白行分离开，映射到表结构 参见python自带函数map() </span></span><br><span class="line">        datas.append(num) <span class="comment">#后续拓展表</span></span><br><span class="line">        <span class="comment">#print(datas)</span></span><br><span class="line"><span class="keyword">finally</span>:</span><br><span class="line">     file_object.close() <span class="comment">#文件流关闭</span></span><br><span class="line"></span><br><span class="line">plt.figure(num = <span class="number">2019</span>, figsize = (<span class="number">5</span>, <span class="number">5</span>)) <span class="comment">#初始化图形，指定figure的编号并指定figure的大小</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> datas:</span><br><span class="line">    a = data[<span class="number">0</span>] <span class="comment"># xi</span></span><br><span class="line">    b = data[<span class="number">1</span>] <span class="comment"># yi</span></span><br><span class="line">    k = b / a</span><br><span class="line">    x = np.linspace(<span class="number">0</span>, a, int(<span class="number">50</span> * a))</span><br><span class="line">    y = k * x</span><br><span class="line">    plt.plot(x,y,color = <span class="string">'black'</span>, linewidth = <span class="number">1.0</span>, linestyle = <span class="string">'--'</span>) <span class="comment">#定义线性</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># y = A * x    E = dA * x     dA= L * (E / x)</span></span><br><span class="line">A = <span class="number">0.1</span></span><br><span class="line">L = <span class="number">0.5</span> <span class="comment">#预设的初始值</span></span><br><span class="line"><span class="comment"># 进行简单的拟合  一个可循环的过程</span></span><br><span class="line"><span class="keyword">for</span> data <span class="keyword">in</span> datas:</span><br><span class="line">    E = data[<span class="number">1</span>] - A * data[<span class="number">0</span>] + data[<span class="number">1</span>] / <span class="number">10</span></span><br><span class="line">    dA = L * (E / data[<span class="number">0</span>])</span><br><span class="line">    A = A + dA</span><br><span class="line">    </span><br><span class="line">x = np.linspace(<span class="number">0</span>, <span class="number">5</span>, <span class="number">250</span>) <span class="comment">#绘制图形</span></span><br><span class="line">y = A * x</span><br><span class="line">plt.plot(x,y,color = <span class="string">'green'</span>, linewidth = <span class="number">1.0</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>有时候一个分类器不足以解决问题，如果数据本身不是单一线性过程支配，那么一个简单的线性分类器不能对数据进行划分。例如逻辑上的异或问题（如下方图）。这样的问题需要多个线性分类器来共同解决</p>
<div align="center">
<img src="https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/post/190721/Figure_2.png" alt="多个线性分类器解决异或问题">
</div>
<h3 id="神经元"><a class="markdownIt-Anchor" href="#神经元"></a> 神经元</h3>
<div align="center">
<img src="https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/post/190721/neuron.jpg" alt="神经元">
</div>
<p>大脑神经的基本单元是神经元，虽然神经元有着多种形式，神经元都会将电信号从一段传递到另一端，沿着突触，将电信号从树突传递到树突。神经元主要结构可以分为树突和终端。一个神经元接受了电信号，当接受信号超过一定阈值（threshold），神经元被激发后链接电路，产生一个输出信号。</p>
<p>思考我们如何建模人工神经。生物神经元可以接受很多输出，我们需要将他们加权相加得到总输入，通过内在的阈值函数计算得到输出结果，可以提供信号给更多的神经元。将这种结构复制到人造模型的一种方法是构造多层神经元，如图所示</p>
<div align="center">
<img src="https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/post/190721/neural%20networks1.jpg" alt="neural networks">
</div>
<p>每一层的节点都与所有下一层的节点通过一定的权重直接相连，通过调整节点之间的连接强度或阈值函数的形状来进行节点之间的强度调节</p>
<p>了解了处理结构之后，激活函数引入给我们的模型带入了非线性元素，可以处理更为复杂的问题，实际中较为常用的激活函数主要有三种，Sigmoid, tanh, 和ReLU</p>
<h4 id="sigmoid"><a class="markdownIt-Anchor" href="#sigmoid"></a> Sigmoid</h4>
<p>（这里是因为我的Katex公式渲染还有一点问题，还得待调试，下面把python的计算代码贴上来）</p>
<blockquote>
<p>y = 1 / (1 + np.exp(-x))</p>
</blockquote>
<div align="center">
<img src="https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/post/190721/Sigmoid%20Function.png" alt="Sigmoid Fuction">
</div>
<p>Sigmoid函数经过激活之后的输出区间为（0，1），输入的数据越大，输出越接近1，否则越接近0。sigmoid函数与生物神经网络的工作机制很类似，在一开始作为激活函数时广泛收到大众认可，但他的缺点也非常明显，他的最大问题在于会导致模型的梯度消失，函数的导数取值区间为0~0.25，在后向传播时，每逆向经过一个节点就要乘以一个sigmoid的导数值，当模型层次达到一定程度时，会导致梯度值越来越小，直到消失。</p>
<h4 id="tanh"><a class="markdownIt-Anchor" href="#tanh"></a> tanh</h4>
<blockquote>
<p>y1 = (np.exp(x) - np.exp(-x)) / (np.exp(x) + np.exp(-x))</p>
</blockquote>
<div align="center">
<img src="https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/post/190721/Tanh%20Function.png" alt="tanh">
</div>
<p>tanh函数的输出结果是中心对称的，解决了激活函数在模型优化中收敛速度变慢的问题，然鹅求导的取值区域为0~1，仍然不够大</p>
<h4 id="relu"><a class="markdownIt-Anchor" href="#relu"></a> ReLU</h4>
<blockquote>
<p>f(x)= max(0,x)</p>
</blockquote>
<div align="center">
<img src="https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/post/190721/ReLU%20Function.png" alt="ReLU">
</div>
<p>ReLU修正线性单元 是现在在深度学习神经网络模型中使用率最高的激活函数，使用它在实际计算中非常高效，收敛速度非常快，但是函数特性可能会导致有一些神经元永远不会被激活，并且这些神经元参数不能被更新。这一般是由于模型参数在初始化的时候采用了全正或者全负的值，或者在后向传播中设置的学习率过高导致的。解决方法有对模型参数使用更高级的初始化方法，比如Xavier，以及合理的后向传播速率，推荐使用自适应的算法比如Adam</p>
<h3 id="追踪信号"><a class="markdownIt-Anchor" href="#追踪信号"></a> 追踪信号</h3>
<p>可以用矩阵的陈发表示所有的运算</p>
<blockquote>
<p><em>X = W * I</em></p>
</blockquote>
<p>W为权重矩阵，以第二层为视界，每一行为第一层的各个节点连接到第二层的某节点的权重序列;I矩阵为输入矩阵，X为组合调节之后的信号</p>
<blockquote>
<p><em>O = function(X)</em></p>
</blockquote>
<p>O表示着经过激活函数处理后的该层的所有输出</p>
<p>以一个结构为3x3的，有input、hidden、output层组成的神经网络作为算例，一次结果的输出过程如下：</p>
<blockquote>
<p>I : input_data<br>
<em>X<sub>hidden</sub> = W<sub>input_hidden</sub> · I</em><br>
<em>O<sub>hidden</sub> = sigmoid(X<sub>hidden</sub>)</em><br>
<em>X<sub>output</sub> = W<sub>hidden_output</sub> · O<sub>hidden</sub></em><br>
<em>O<sub>output</sub> = X<sub>output</sub></em></p>
</blockquote>
<h3 id="后向传播"><a class="markdownIt-Anchor" href="#后向传播"></a> 后向传播</h3>
<h3 id="更新权重"><a class="markdownIt-Anchor" href="#更新权重"></a> 更新权重</h3>
<h2 id="python神经网络编程"><a class="markdownIt-Anchor" href="#python神经网络编程"></a> python神经网络编程</h2>
<p>py搭建简单的三层神经网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> scipy.special</span><br><span class="line"><span class="comment">#定义三层神经网络</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">neuralNetwork</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, inputnodes, hiddennodes, outputnodes, learningrate)</span>:</span></span><br><span class="line">        self.inodes = inputnodes <span class="comment">#输入层</span></span><br><span class="line">        self.hnodes = hiddennodes <span class="comment">#隐藏层</span></span><br><span class="line">        self.onodes = outputnodes <span class="comment">#输出层</span></span><br><span class="line">        self.lr = learningrate</span><br><span class="line">        <span class="comment">#较为复杂的权重   中心值为0 标准差为传入节点数开方的倒数(-0.5次方) 的正态分布 </span></span><br><span class="line">        self.wih = np.random.normal(<span class="number">0.0</span>, pow(self.hnodes, <span class="number">-0.5</span>), (self.hnodes, self.inodes))  <span class="comment">#100 278</span></span><br><span class="line">        self.who = np.random.normal(<span class="number">0.0</span>, pow(self.onodes, <span class="number">-0.5</span>), (self.onodes, self.hnodes))  <span class="comment">#10 100</span></span><br><span class="line"></span><br><span class="line">        self.activation_function = <span class="keyword">lambda</span> x: scipy.special.expit(x)</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(self, inputs_list, targets_list)</span>:</span></span><br><span class="line"></span><br><span class="line">        inputs = np.array(inputs_list, ndmin = <span class="number">2</span>).T  <span class="comment">#278*1</span></span><br><span class="line">        targets = np.array(targets_list, ndmin = <span class="number">2</span>).T <span class="comment"># 10*1</span></span><br><span class="line">        </span><br><span class="line">        hidden_inputs = np.dot(self.wih, inputs) <span class="comment">#100 1</span></span><br><span class="line">        </span><br><span class="line">        hidden_outputs = self.activation_function(hidden_inputs) <span class="comment">#100 1</span></span><br><span class="line"></span><br><span class="line">        final_inputs = np.dot(self.who, hidden_outputs) <span class="comment">#10 1</span></span><br><span class="line"></span><br><span class="line">        final_outputs = self.activation_function(final_inputs)  <span class="comment"># 10 1</span></span><br><span class="line"></span><br><span class="line">        output_errors = targets - final_outputs <span class="comment"># 10 1</span></span><br><span class="line"></span><br><span class="line">        hidden_errors = np.dot(self.who.T , output_errors) <span class="comment"># 100 1</span></span><br><span class="line">        <span class="comment">#更新层之间的权重 dWij = alpha * Ek * S(Ok) * (1 - S(Ok)) * Oj.T</span></span><br><span class="line">        self.who += self.lr * np.dot((output_errors * final_outputs * (<span class="number">1.0</span> - final_outputs)), np.transpose(hidden_outputs))</span><br><span class="line">        self.wih += self.lr * np.dot((hidden_errors * hidden_outputs * (<span class="number">1.0</span> - hidden_outputs)), np.transpose(inputs))</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">query</span><span class="params">(self, inputs_list)</span>:</span></span><br><span class="line"></span><br><span class="line">        inputs = np.array(inputs_list, ndmin = <span class="number">2</span>).T   </span><br><span class="line">        hidden_inputs = np.dot(self.wih, inputs)</span><br><span class="line">        hidden_outputs = self.activation_function(hidden_inputs)</span><br><span class="line">        final_inputs = np.dot(self.who, hidden_outputs)</span><br><span class="line">        final_outputs = self.activation_function(final_inputs)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> final_outputs</span><br><span class="line"></span><br><span class="line"><span class="comment">#set number</span></span><br><span class="line">input_nodes = <span class="number">784</span></span><br><span class="line">hidden_nodes = <span class="number">100</span></span><br><span class="line">output_nodes = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">learning_rate = <span class="number">0.3</span></span><br><span class="line"><span class="comment">#create instance of neural network</span></span><br><span class="line">n = neuralNetwork(input_nodes, hidden_nodes, output_nodes, learning_rate)</span><br><span class="line"></span><br><span class="line"><span class="comment">#train the neural network</span></span><br><span class="line">training_data_file = open(<span class="string">"mnist_train.csv"</span>,<span class="string">'r'</span>)</span><br><span class="line">training_data_list = training_data_file.readlines()</span><br><span class="line">training_data_file.close()</span><br><span class="line">i = <span class="number">1</span></span><br><span class="line"><span class="keyword">for</span> record <span class="keyword">in</span> training_data_list:</span><br><span class="line">    all_values = record.split(<span class="string">','</span>)</span><br><span class="line">    inputs = (np.asfarray(all_values[<span class="number">1</span>:]) / <span class="number">255.0</span> * <span class="number">0.99</span>) + <span class="number">0.01</span></span><br><span class="line">    <span class="comment">#print('正在训练第' + str(i) + '个')</span></span><br><span class="line">    i += <span class="number">1</span></span><br><span class="line">    targets = np.zeros(output_nodes) + <span class="number">0.01</span></span><br><span class="line">    targets[int(all_values[<span class="number">0</span>])] = <span class="number">0.99</span>   <span class="comment">#1*10</span></span><br><span class="line">    n.train(inputs, targets)</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line"><span class="comment">#print('训练结束\n')</span></span><br><span class="line"><span class="comment">#test the network</span></span><br><span class="line"></span><br><span class="line">test_data_file = open(<span class="string">"mnist_test.csv"</span>,<span class="string">'r'</span>)</span><br><span class="line">test_data_list = test_data_file.readlines()</span><br><span class="line">test_data_file.close()</span><br><span class="line">score = <span class="number">0</span></span><br><span class="line">num = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> record <span class="keyword">in</span> test_data_list:</span><br><span class="line">    all_values = record.split(<span class="string">','</span>)</span><br><span class="line"><span class="comment">##    image_aarray = np.asfarray(all_values[1:]).reshape((28,28))</span></span><br><span class="line"><span class="comment">##    plt.imshow(image_aarray, cmap = 'Greys', interpolation = 'None')</span></span><br><span class="line"><span class="comment">##    plt.show()</span></span><br><span class="line">    correct_label = int(all_values[<span class="number">0</span>])</span><br><span class="line">    outputs = n.query((np.asfarray(all_values[<span class="number">1</span>:]) / <span class="number">255.0</span> * <span class="number">0.99</span>) + <span class="number">0.01</span>)</span><br><span class="line">    label = np.argmax(outputs)</span><br><span class="line">    <span class="comment">#print('correct:' + str(correct_label) +' result:' + str(label))</span></span><br><span class="line">    <span class="comment">#下面是随便写的  凑合着看看</span></span><br><span class="line">    <span class="keyword">if</span> label == correct_label:</span><br><span class="line">        score += <span class="number">1</span></span><br><span class="line">        num +=<span class="number">1</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        num += <span class="number">1</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">print(<span class="string">'score:'</span>+ str(score))</span><br><span class="line">print(<span class="string">'num:'</span> + str(num))</span><br><span class="line">print(<span class="string">'rate:'</span> + str(score/num))</span><br></pre></td></tr></table></figure>
<h2 id="深度学习框架下的神经网络编程"><a class="markdownIt-Anchor" href="#深度学习框架下的神经网络编程"></a> 深度学习框架下的神经网络编程</h2>
<h3 id="pytorch"><a class="markdownIt-Anchor" href="#pytorch"></a> pytorch</h3>
<h4 id="pytorch环境配置"><a class="markdownIt-Anchor" href="#pytorch环境配置"></a> Pytorch环境配置</h4>
<h5 id="env_torch配置"><a class="markdownIt-Anchor" href="#env_torch配置"></a> env_torch配置</h5>
<p>如果要安装可以在GPU上运行的版本，需要电脑配有NVDIA显卡，并且提前安装CUDA比较先进的版本<br>
在anaconda的监视器界面中，新建一个包含适合pytorch工程的新环境，命名为env_tf，直接选择复制了tensorflow的环境然后把tf卸载掉<br>
由于conda指令下载缓慢，选择pip下载，在环境prompt中分别输入</p>
<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">pip install https://download.pytorch.org/whl/cu100/torch-1.1.0-cp37-cp37m-win_amd64.whl</span><br><span class="line">pip install https://download.pytorch.org/whl/cu100/torchvision-0.3.0-cp37-cp37m-win_amd64.whl</span><br><span class="line">```   </span><br><span class="line">由于conda下载速度缓慢，建议使用加速器</span><br><span class="line"><span class="comment">##### jupyter notebook</span></span><br><span class="line">该IDE环境使用较多，可在env_torch内通过下方指令安装，建议连加速器</span><br><span class="line">```bash</span><br><span class="line">conda install jupyter</span><br></pre></td></tr></table></figure>
<h3 id="tensorflow"><a class="markdownIt-Anchor" href="#tensorflow"></a> tensorflow</h3>
<h2 id="卷积神经网络"><a class="markdownIt-Anchor" href="#卷积神经网络"></a> 卷积神经网络</h2>
<h3 id="卷积神经网络实战"><a class="markdownIt-Anchor" href="#卷积神经网络实战"></a> 卷积神经网络实战</h3>
<p>以简单的mnist手写字符训练集为例子</p>
<p>tensorflow实现的简单卷积神经网络</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> tensorflow.examples.tutorials.mnist <span class="keyword">import</span> input_data</span><br><span class="line"><span class="keyword">import</span> tensorflow <span class="keyword">as</span> tf</span><br><span class="line"></span><br><span class="line"><span class="comment">#加载原始数据</span></span><br><span class="line">mnist = input_data.read_data_sets(<span class="string">'C:/User/sheld/Desktop/data/'</span>, one_hot = <span class="literal">True</span>)</span><br><span class="line">sess = tf.InteractiveSession()</span><br><span class="line"></span><br><span class="line"><span class="comment">#初始化权重和偏值函数  并加入噪声</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">weight_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.truncated_normal(shape, stddev = <span class="number">0.1</span>)  <span class="comment">#正态分布  噪声标准差设为0.1</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial) </span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">bias_variable</span><span class="params">(shape)</span>:</span></span><br><span class="line">    initial = tf.constant(<span class="number">0.1</span>, shape = shape) <span class="comment">#正值 0.1    return tf.Variable(initial)</span></span><br><span class="line">    <span class="keyword">return</span> tf.Variable(initial)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义卷积层</span></span><br><span class="line"><span class="comment">#x：输入</span></span><br><span class="line"><span class="comment">#W：卷积参数 [a, b, c, d] a b 表示卷积核尺寸，c表示通道数， d表示卷积核数</span></span><br><span class="line"><span class="comment">#strides 步长</span></span><br><span class="line"><span class="comment">#padding 边界处理方式  same:输入和输出保持同样的尺寸</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">conv2d</span><span class="params">(x, W)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.conv2d(x, W, strides = [<span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>, <span class="number">1</span>], padding = <span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义池化层</span></span><br><span class="line"><span class="comment">#2x2最大化池 缩小像素  把横竖步长设为2</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_pool_2x2</span><span class="params">(x)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> tf.nn.max_pool(x, ksize = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>], strides = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>],</span><br><span class="line">                          padding = <span class="string">'SAME'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">#转化成2d结构  x是输入  y_是真实label</span></span><br><span class="line">x = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">784</span>]) <span class="comment">#输入的数据占位符</span></span><br><span class="line">y_ = tf.placeholder(tf.float32, [<span class="literal">None</span>, <span class="number">10</span>]) <span class="comment">#输入的标签占位符</span></span><br><span class="line">x_image = tf.reshape(x, [<span class="number">-1</span>, <span class="number">28</span>, <span class="number">28</span>, <span class="number">1</span>])</span><br><span class="line"></span><br><span class="line"><span class="comment">#卷积层1</span></span><br><span class="line">W_conv1 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">1</span>,<span class="number">32</span>])</span><br><span class="line">b_conv1 = bias_variable([<span class="number">32</span>])</span><br><span class="line">h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)</span><br><span class="line">h_pool1 = max_pool_2x2(h_conv1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#卷积层2</span></span><br><span class="line">W_conv2 = weight_variable([<span class="number">5</span>,<span class="number">5</span>,<span class="number">32</span>,<span class="number">64</span>])</span><br><span class="line">b_conv2 = bias_variable([<span class="number">64</span>])</span><br><span class="line">h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)</span><br><span class="line">h_pool2 = max_pool_2x2(h_conv2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#全连接层</span></span><br><span class="line">W_fc1 = weight_variable([<span class="number">7</span> * <span class="number">7</span> * <span class="number">64</span>, <span class="number">1024</span>])</span><br><span class="line">b_fc1 = bias_variable([<span class="number">1024</span>])</span><br><span class="line">h_pool2_flat = tf.reshape(h_pool2, [<span class="number">-1</span>, <span class="number">7</span>*<span class="number">7</span>*<span class="number">64</span>])</span><br><span class="line">h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Dropout层</span></span><br><span class="line">keep_prob = tf.placeholder(tf.float32)</span><br><span class="line">h_fc1_drop = tf.nn.dropout(h_fc1, keep_prob)</span><br><span class="line"></span><br><span class="line"><span class="comment">#Softmax层</span></span><br><span class="line">W_fc2 = weight_variable([<span class="number">1024</span>, <span class="number">10</span>])</span><br><span class="line">b_fc2 = bias_variable([<span class="number">10</span>])</span><br><span class="line">y_conv = tf.nn.softmax(tf.matmul(h_fc1_drop, W_fc2) + b_fc2)</span><br><span class="line"></span><br><span class="line"><span class="comment">#定义损失函数  应用Adam优化器</span></span><br><span class="line">cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y_conv),</span><br><span class="line">                                              reduction_indices = [<span class="number">1</span>])) <span class="comment">#交叉熵</span></span><br><span class="line">train_step = tf.train.AdamOptimizer(<span class="number">1e-4</span>).minimize(cross_entropy) <span class="comment">#梯度下降法</span></span><br><span class="line"></span><br><span class="line"><span class="comment">#评定精准率</span></span><br><span class="line">correct_prediction = tf.equal(tf.argmax(y_conv, <span class="number">1</span>), tf.argmax(y_, <span class="number">1</span>))  <span class="comment">#精确度计算</span></span><br><span class="line">accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))</span><br><span class="line"></span><br><span class="line"><span class="comment">#开始训练</span></span><br><span class="line">tf.global_variables_initializer().run()</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">20000</span>):</span><br><span class="line">    batch = mnist.train.next_batch(<span class="number">50</span>)</span><br><span class="line">    <span class="keyword">if</span> <span class="number">1</span> % <span class="number">100</span> ==<span class="number">0</span>:</span><br><span class="line">        train_accuracy = accuracy.eval(</span><br><span class="line">            feed_dict = &#123;x:batch[<span class="number">0</span>], y_:batch[<span class="number">1</span>], keep_prob: <span class="number">1.0</span>&#125;)</span><br><span class="line">        print(<span class="string">'step %d, training accuracy %g'</span>%(i,train_accuracy))</span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line">    train_step.run(feed_dict = &#123;x:batch[<span class="number">0</span>], y_:batch[<span class="number">1</span>], keep_prob: <span class="number">0.5</span>&#125;)</span><br><span class="line">    <span class="keyword">pass</span></span><br><span class="line">print(<span class="string">'test accuracy %g'</span> % accuracy.eval(</span><br><span class="line">    feed_dict = &#123;x:mnist.test.images, y_:mnist.test.labels, keep_prob: <span class="number">1.0</span>&#125;))</span><br></pre></td></tr></table></figure>
<p>pytorch实现的版本</p>
<pre class="highlight"><code class="python"></code></pre>

            <!--[if lt IE 9]><script>document.createElement('audio');</script><![endif]-->
            <audio id="audio" loop="1" preload="auto" controls="controls" data-autoplay="false">
                <source type="audio/mpeg" src="">
            </audio>
            
                <ul id="audio-list" style="display:none">
                    
                        <li title='0' data-url='https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/DDBY%20-%20%E9%9D%92%E3%81%84%E7%A9%BA%E3%80%81%E7%99%BD%E3%81%84%E9%9B%B2%E3%80%82%E6%B5%81%E3%82%8C%E3%82%8B%E9%A2%A8%E3%81%8C%E5%BF%83%E5%9C%B0%E8%89%AF%E3%81%8F.mp3'></li>
                    
                        <li title='1' data-url='https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/DDBY%20-%20Starry%20Memories.mp3'></li>
                    
                        <li title='2' data-url='https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/DDBY%20-%20Starry%20Memories.mp3'></li>
                    
                        <li title='3' data-url='https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/TAMUSIC%20-%20%E5%90%91%E3%81%93%E3%81%86%E4%BE%A7%E3%81%AE%E6%9C%88.mp3'></li>
                    
                        <li title='4' data-url='https://antrovirens-1-1258258000.cos.ap-shanghai.myqcloud.com/%E4%B8%8A%E6%B5%B7%E3%82%A2%E3%83%AA%E3%82%B9%E5%B9%BB%E6%A8%82%E5%9B%A3%20-%20%E5%90%91%E3%81%93%E3%81%86%E5%81%B4%E3%81%AE%E6%9C%88.mp3'></li>
                    
                </ul>
            
        </div>
        
    <div id='gitalk-container' class="comment link"
        data-ae='false'
        data-ci='aa8bb51537465fdc96a8'
        data-cs='42a5b0db726d5fd6dd5baef6f452c419ff2634a4'
        data-r='antrovirens.github.io'
        data-o='antrovirens'
        data-a='antrovirens'
        data-d='false'
    >查看评论</div>


    </div>
    
</div>


    </div>
</div>
</body>
<script src="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js"></script>
<script src="//lib.baomitu.com/jquery/1.8.3/jquery.min.js"></script>
<script src="/js/plugin.js"></script>
<script src="/js/diaspora.js"></script>
<link rel="stylesheet" href="/photoswipe/photoswipe.css">
<link rel="stylesheet" href="/photoswipe/default-skin/default-skin.css">
<script src="/photoswipe/photoswipe.min.js"></script>
<script src="/photoswipe/photoswipe-ui-default.min.js"></script>

<!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">
    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>
    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">
        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>
        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">
            <div class="pswp__top-bar">
                <!--  Controls are self-explanatory. Order can be changed. -->
                <div class="pswp__counter"></div>
                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
                <button class="pswp__button pswp__button--share" title="Share"></button>
                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>
            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>
            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>
            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>
            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>
        </div>
    </div>
</div>




</html>
